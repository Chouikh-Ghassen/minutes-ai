{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSJNAqs4wwKj"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import tempfile, numpy as np, soundfile as sf\n",
        "from transformers import TextIteratorStreamer\n",
        "import threading"
      ],
      "metadata": {
        "id": "bFBvjWLZxDLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "transcription_model = \"gpt-4o-mini-transcribe\"\n",
        "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "#LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "id": "Y25o-GcYxDI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "# --- Device + dtype config (single source of truth) ---\n",
        "USE_CUDA       = torch.cuda.is_available()\n",
        "TORCH_DEVICE   = \"cuda\" if USE_CUDA else \"cpu\"   # for tensors / .to()\n",
        "TORCH_DTYPE    = torch.float16 if USE_CUDA else torch.float32\n",
        "ASR_DTYPE      = torch.float16 if USE_CUDA else None"
      ],
      "metadata": {
        "id": "3HQ6c2xb3iWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA  = torch.cuda.is_available()\n",
        "# Detect if CUDA (GPU) is available\n",
        "PIPE_DEVICE = 0 if USE_CUDA else -1           # for HF pipeline(device=â€¦)-1\n",
        "# Set dtype only if weâ€™re on GPU\n",
        "PIPE_dtype = torch.float16 if USE_CUDA else None # pipeline(dtype=â€¦); None on CPU\n",
        "TORCH_DTYPE    = torch.float16 if use_cuda else torch.float32\n",
        "\n",
        "#Use Open Source for Transcription - Hugging Face Pipelines ( transcription : from audio to text)\n",
        "pipe = pipeline(\"automatic-speech-recognition\",model=\"openai/whisper-medium.en\", dtype=PIPE_dtype, device=PIPE_DEVICE, return_timestamps=True)"
      ],
      "metadata": {
        "id": "aoxdmfYY-CoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcription_fn(audio_file, progress=gr.Progress()):\n",
        "    result = pipe(audio_file)\n",
        "    transcription = result[\"text\"]\n",
        "    return transcription"
      ],
      "metadata": {
        "id": "K6SVUSL_xDGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "7Ilygc9_AULL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ") if use_cuda else None"
      ],
      "metadata": {
        "id": "bAsQBVJmBLGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "streamer = TextStreamer(tokenizer)\n",
        "# Initialize Llama model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLAMA,\n",
        "    device_map=\"auto\" if use_cuda else None,\n",
        "    quantization_config=quant_config,\n",
        "    torch_dtype=torch.float16 if use_cuda else torch.float32\n",
        ")\n",
        "if not use_cuda:\n",
        "    model.to(\"cpu\")"
      ],
      "metadata": {
        "id": "Kc83XWNBBMI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def message_LLAMA(audio_file, progress=gr.Progress()):\n",
        "\n",
        "#   transcription =transcription_fn(audio_file)\n",
        "#   progress(0.6, desc=\"Generating meeting minutes from transcript...\")\n",
        "#   system_message = \"\"\"\n",
        "#   You produce minutes of meetings from transcripts, with summary, key discussion points,\n",
        "#   takeaways and action items with owners, in markdown format without code blocks.\n",
        "#   \"\"\"\n",
        "\n",
        "#   user_prompt = f\"\"\"\n",
        "#   Below is an extract transcript of a Denver council meeting.\n",
        "#   Please write minutes in markdown without code blocks, including:\n",
        "#   - a summary with attendees, location and date\n",
        "#   - discussion points\n",
        "#   - takeaways\n",
        "#   - action items with owners\n",
        "\n",
        "#   Transcription:\n",
        "#   {transcription}\n",
        "#   \"\"\"\n",
        "\n",
        "#   messages = [\n",
        "#       {\"role\": \"system\", \"content\": system_message},\n",
        "#       {\"role\": \"user\", \"content\": user_prompt}\n",
        "#     ]\n",
        "\n",
        "#   inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "#   outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
        "#   response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#   return response # this is important for Gradio,\n",
        "#                   # if you want to test the function without gradio, you can delete it"
      ],
      "metadata": {
        "id": "g0ra1rPOxDDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def message_LLAMA(audio_file, progress=gr.Progress()):\n",
        "\n",
        "    \"\"\"\n",
        "  # ðŸ”¹ Handle case: (sample_rate, numpy_array)\n",
        "  if isinstance(audio_file, tuple) and len(audio_file) == 2:\n",
        "      sr, data = audio_file\n",
        "      if isinstance(data, np.ndarray):\n",
        "          tmp_wav = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "          sf.write(tmp_wav.name, data, sr)\n",
        "          audio_file = tmp_wav.name  # overwrite with path\n",
        "    \"\"\"\n",
        "    progress(0.3, desc=\"Creating transcript from audio...\")\n",
        "    transcription = transcription_fn(audio_file)\n",
        "\n",
        "    progress(0.6, desc=\"Generating meeting minutes from transcript...\")\n",
        "    system_message = (\n",
        "        \"You produce minutes of meetings from transcripts, with summary, key discussion points, \"\n",
        "        \"takeaways and action items with owners, in markdown format without code blocks.\"\n",
        "    )\n",
        "    user_prompt = f\"\"\"\n",
        "    Below is an extract transcript of a Denver council meeting.\n",
        "    Please write minutes in markdown without code blocks, including:\n",
        "    - a summary with attendees, location and date\n",
        "    - discussion points\n",
        "    - takeaways\n",
        "    - action items with owners\n",
        "\n",
        "    Transcription:\n",
        "    {transcription}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # No streamer here\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response # this is important for Gradio,\n",
        "                  # if you want to test the function without gradio, you can delete it"
      ],
      "metadata": {
        "id": "DE2KuIDnPekE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get access to the mp3 file.\n",
        "drive.mount(\"/content/drive\")\n",
        "audio_filename = \"/content/drive/MyDrive/denver_extract.mp3\"\n",
        "audio_file = open(audio_filename, \"rb\")"
      ],
      "metadata": {
        "id": "hcQbtsIyxC3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function without Gradio\n",
        "message_LLAMA(audio_filename)"
      ],
      "metadata": {
        "id": "iYh3iMc0DCkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio without Streaming"
      ],
      "metadata": {
        "id": "3mNZTevIQfgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio interface\n",
        "\n",
        "view = gr.Interface(\n",
        "    fn=message_LLAMA,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload MP3 File\", format=\"mp3\"),\n",
        "    #outputs=gr.Markdown(label=\"Meeting Minutes\", min_height=60),\n",
        "    outputs=gr.Textbox(label=\"Meeting Minutes (Markdown)\", lines=12),\n",
        "    title=\"Meeting Minutes Generator\",\n",
        "    description=\"Upload an MP3 recording of your meeting to get AI-generated meeting minutes. This process may take a few minutes.\",\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "\n",
        "view.launch()"
      ],
      "metadata": {
        "id": "6V_S6mc4Dnuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming into Gradio :"
      ],
      "metadata": {
        "id": "fu3iJOUjQVVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "import threading\n",
        "\n",
        "def message_LLAMA_stream(audio_file, progress=gr.Progress()):\n",
        "    progress(0.3, desc=\"Creating transcript from audio...\")\n",
        "    transcription = transcription_fn(audio_file)\n",
        "\n",
        "    progress(0.6, desc=\"Generating meeting minutes from transcript...\")\n",
        "    system_message = (\n",
        "        \"You produce minutes of meetings from transcripts, with summary, key discussion points, \"\n",
        "        \"takeaways and action items with owners, in markdown format without code blocks.\"\n",
        "    )\n",
        "    user_prompt = f\"\"\"\n",
        "    Below is an extract transcript of a Denver council meeting.\n",
        "    Please write minutes in markdown without code blocks, including:\n",
        "    - a summary with attendees, location and date\n",
        "    - discussion points\n",
        "    - takeaways\n",
        "    - action items with owners\n",
        "\n",
        "    Transcription:\n",
        "    {transcription}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Create a fresh iterator streamer per request\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    # Run generation in a thread so we can iterate over streamer\n",
        "    gen_kwargs = dict(inputs=inputs, max_new_tokens=2000, streamer=streamer)\n",
        "    thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    partial = \"\"\n",
        "    for new_text in streamer:\n",
        "        partial += new_text\n",
        "        yield partial  # Gradio updates the textbox live\n"
      ],
      "metadata": {
        "id": "BC10_UDXQQuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view = gr.Interface(\n",
        "    fn=message_LLAMA_stream,  # ðŸ‘ˆ use the stream fn\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload MP3 File\"),\n",
        "    outputs=gr.Textbox(label=\"Meeting Minutes (Markdown)\", lines=12),\n",
        "    title=\"Meeting Minutes Generator (Streaming)\",\n",
        "    description=\"Upload an MP3 or record audio. Transcription + live generation.\",\n",
        "    flagging_mode=\"never\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "4JbRNdBaQUXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_6Xyf8-fAHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}